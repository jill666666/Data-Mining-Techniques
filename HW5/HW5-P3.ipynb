{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python381jvsc74a57bd0aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49",
   "display_name": "Python 3.8.1 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
    "from elasticsearch import Elasticsearch\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(['https://sunho:Dunkel6eit!!@i-o-optimized-deployment-84c1c6.es.us-east-1.aws.found.io:9243'], timeout=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_name_to_label_map(filepath):\n",
    "    \"\"\" Map category name to index value. \"\"\"\n",
    "    label_map = {}\n",
    "    category_names = os.listdir(filepath)\n",
    "    for index, category_name in enumerate(category_names):\n",
    "        label_map[category_name] = index\n",
    "    return label_map\n",
    "\n",
    "def create_training_data_and_labels(es, index_name, label_map):\n",
    "    \"\"\" Create training data/labels given the Elasticsearch client. \"\"\"\n",
    "    doc_ids, training_data, training_labels = [], [], []\n",
    "    res = es.search(index=index_name, size=20000)\n",
    "    hits = res['hits']['hits']\n",
    "    for hit in tqdm(hits, position=0, desc='generating training data/labels'):\n",
    "        doc_id = hit['_id']\n",
    "        text = hit['_source']['text']\n",
    "        category = hit['_source']['category']\n",
    "        doc_ids.append(doc_id)\n",
    "        training_data.append(text)\n",
    "        training_labels.append(label_map[category])\n",
    "    return doc_ids, np.array(training_data), training_labels\n",
    "\n",
    "def create_and_train_tokenizer(train_file_ids):\n",
    "    text = \"\"\n",
    "    for file_id in train_file_ids:\n",
    "        text += gutenberg.raw(file_id)\n",
    "\n",
    "    trainer = PunktTrainer()\n",
    "    trainer.INCLUDE_ALL_COLLOCS = True\n",
    "    trainer.train(text)\n",
    "    \n",
    "    tokenizer = PunktSentenceTokenizer(trainer.get_params())\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "generating training data/labels: 100%|██████████| 15417/15417 [00:00<00:00, 306745.97it/s]\n"
     ]
    }
   ],
   "source": [
    "label_map = category_name_to_label_map('./20NG/20news-bydate-train')\n",
    "doc_ids, training_data, training_labels = create_training_data_and_labels(es, '20-ng', label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = create_and_train_tokenizer(train_file_ids=gutenberg.fileids())\n",
    "vectorizer = TfidfVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_score(p, q, lambda_param=0.1):\n",
    "    \"\"\" Compute KL similarity score. \"\"\"\n",
    "    return sum([p[i] * math.log(p[i] + lambda_param / (q[i] + lambda_param * len(p) ) ) for i in range(len(p))])\n",
    "\n",
    "def KL_similarization(training_data, vectorizer, tokenizer):\n",
    "    \"\"\" Extract KL summaries given the training dataset. \"\"\"\n",
    "    summaries = {}\n",
    "\n",
    "    for doc_index, document in enumerate(tqdm(training_data, position=0, desc='KL summarization')):\n",
    "        best_sentences = []\n",
    "        vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        doc_vectors = vectorizer.fit_transform([document]).toarray()[0]\n",
    "        for _ in range(3):\n",
    "            sentences = tokenizer.tokenize(document)\n",
    "            sentence_score_map = {}\n",
    "            for sentence_order, sentence in enumerate(sentences):\n",
    "                if sentence in best_sentences:\n",
    "                    continue\n",
    "                candidates = [summary for summary in best_sentences]\n",
    "                candidates.append(sentence)\n",
    "                candidate_as_sentences = ' '.join(candidate for candidate in candidates)\n",
    "                candidate_vector = vectorizer.transform([candidate_as_sentences]).toarray()[0]\n",
    "                sentence_score_map[sentence_order] = kl_score(doc_vectors, candidate_vector)\n",
    "            if len(sentence_score_map) != 0:\n",
    "                top_candidate_order, top_candidate_score = sorted(sentence_score_map.items(), key=lambda x: x[1])[0]\n",
    "                top_candidate_sentence = sentences[top_candidate_order]\n",
    "                best_sentences.append(top_candidate_sentence)\n",
    "        \n",
    "        doc_id = doc_ids[doc_index]\n",
    "        summary = ' '.join(best_sentences)\n",
    "        summaries[doc_id] = summary\n",
    "    return summaries\n",
    "\n",
    "def LDA_summarization(training_data, vectorizer, tokenizer):\n",
    "    \"\"\" Extract LDA summaries given the training dataset. \"\"\"\n",
    "    summaries = {}\n",
    "    lda = LatentDirichletAllocation(n_components=20, max_iter=5, learning_method='online', learning_offset=50, random_state=0)\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        for doc_index, document in enumerate(tqdm(training_data, position=0, desc='LDA summarization')):\n",
    "            best_sentences = []\n",
    "            vectorizer = TfidfVectorizer(stop_words='english')\n",
    "            doc_vectors = vectorizer.fit_transform([document]).todense()\n",
    "            doc_topics_distribution = lda.fit_transform(doc_vectors).flatten()\n",
    "            for _ in range(3):\n",
    "                sentences = tokenizer.tokenize(document)\n",
    "                sentence_score_map = {}\n",
    "                for sentence_order, sentence in enumerate(sentences):\n",
    "                    if sentence in best_sentences:\n",
    "                        continue\n",
    "                    candidates = [summary for summary in best_sentences]\n",
    "                    candidates.append(sentence)\n",
    "                    candidate_as_sentences = ' '.join(candidate for candidate in candidates)\n",
    "                    candidate_vector = vectorizer.transform([candidate_as_sentences]).todense()\n",
    "                    sentence_topics_distribution = lda.transform(candidate_vector).flatten()\n",
    "                    sentence_score_map[sentence_order] = kl_score(doc_topics_distribution, sentence_topics_distribution)\n",
    "                if len(sentence_score_map) == 0:\n",
    "                    continue\n",
    "                top_candidate_order, top_candidate_score = sorted(sentence_score_map.items(), key=lambda x: x[1])[0]\n",
    "                top_candidate_sentence = sentences[top_candidate_order]\n",
    "                best_sentences.append(top_candidate_sentence)\n",
    "            \n",
    "            doc_id = doc_ids[doc_index]\n",
    "            summary = ' '.join(best_sentences)\n",
    "            summaries[doc_id] = summary\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "KL summarization: 100%|██████████| 15417/15417 [15:02<00:00, 17.09it/s]\n"
     ]
    }
   ],
   "source": [
    "kl_summaries = KL_similarization(training_data, vectorizer, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "LDA summarization: 100%|██████████| 15417/15417 [14:05<00:00, 18.22it/s]\n"
     ]
    }
   ],
   "source": [
    "lda_summaries = LDA_summarization(training_data, vectorizer, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_summaries(index_name, kl_summaries, lda_summaries):\n",
    "    summaries_zipped = zip(kl_summaries.items(), lda_summaries.items())\n",
    "    for (doc_id, kl_summary), (_, lda_summary) in tqdm(summaries_zipped, position=0, desc=f'updating {index_name} summaries'):\n",
    "        yield {\n",
    "            '_index': index_name,\n",
    "            '_op_type': 'update',\n",
    "            '_id': doc_id,\n",
    "            'doc': {\n",
    "                'kl_summary': kl_summary,\n",
    "                'lda_summary': lda_summary\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "updating 20-ng summaries: 15417it [01:13, 211.14it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(15417, [])"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "bulk(es, update_summaries('20-ng', kl_summaries, lda_summaries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'rouge_1_f_score': 0.77586,\n 'rouge_1_f_score_cb': 0.77586,\n 'rouge_1_f_score_ce': 0.77586,\n 'rouge_1_precision': 0.88235,\n 'rouge_1_precision_cb': 0.88235,\n 'rouge_1_precision_ce': 0.88235,\n 'rouge_1_recall': 0.69231,\n 'rouge_1_recall_cb': 0.69231,\n 'rouge_1_recall_ce': 0.69231,\n 'rouge_2_f_score': 0.57894,\n 'rouge_2_f_score_cb': 0.57894,\n 'rouge_2_f_score_ce': 0.57894,\n 'rouge_2_precision': 0.66,\n 'rouge_2_precision_cb': 0.66,\n 'rouge_2_precision_ce': 0.66,\n 'rouge_2_recall': 0.51562,\n 'rouge_2_recall_cb': 0.51562,\n 'rouge_2_recall_ce': 0.51562,\n 'rouge_3_f_score': 0.5,\n 'rouge_3_f_score_cb': 0.5,\n 'rouge_3_f_score_ce': 0.5,\n 'rouge_3_precision': 0.57143,\n 'rouge_3_precision_cb': 0.57143,\n 'rouge_3_precision_ce': 0.57143,\n 'rouge_3_recall': 0.44444,\n 'rouge_3_recall_cb': 0.44444,\n 'rouge_3_recall_ce': 0.44444,\n 'rouge_4_f_score': 0.45455,\n 'rouge_4_f_score_cb': 0.45455,\n 'rouge_4_f_score_ce': 0.45455,\n 'rouge_4_precision': 0.52083,\n 'rouge_4_precision_cb': 0.52083,\n 'rouge_4_precision_ce': 0.52083,\n 'rouge_4_recall': 0.40323,\n 'rouge_4_recall_cb': 0.40323,\n 'rouge_4_recall_ce': 0.40323,\n 'rouge_su4_f_score': 0.64759,\n 'rouge_su4_f_score_cb': 0.64759,\n 'rouge_su4_f_score_ce': 0.64759,\n 'rouge_su4_precision': 0.74138,\n 'rouge_su4_precision_cb': 0.74138,\n 'rouge_su4_precision_ce': 0.74138,\n 'rouge_su4_recall': 0.57487,\n 'rouge_su4_recall_cb': 0.57487,\n 'rouge_su4_recall_ce': 0.57487}\n"
     ]
    }
   ],
   "source": [
    "from pyrouge.rouge import Rouge155\n",
    "from pprint import pprint\n",
    "\n",
    "ref_texts = {'A': \"Poor nations pressurise developed countries into granting trade subsidies.\"}\n",
    "summary_text = \"Poor nations demand trade subsidies from developed nations.\"\n",
    "\n",
    "\n",
    "rouge = Rouge155(n_words=100)\n",
    "score = rouge.score_summary(summary_text, ref_texts)\n",
    "pprint(score)"
   ]
  }
 ]
}