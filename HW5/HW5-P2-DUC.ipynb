{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python381jvsc74a57bd0aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49",
   "display_name": "Python 3.8.1 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "source": [
    "### Elasticsearch Client"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(['https://sunho:Dunkel6eit!!@i-o-optimized-deployment-84c1c6.es.us-east-1.aws.found.io:9243'], timeout=30)"
   ]
  },
  {
   "source": [
    "### Generate Training Data and Labels"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data_and_labels(es, index_name):\n",
    "    \"\"\" Create training data/labels given the Elasticsearch client. \"\"\"\n",
    "    doc_ids, training_data, training_labels = [], [], []\n",
    "    res = es.search(index=index_name, size=20000)\n",
    "    hits = res['hits']['hits']\n",
    "    for hit in tqdm(hits, position=0, desc='creating training data/labels'):\n",
    "        doc_id = hit['_id']\n",
    "        text = hit['_source']['text']\n",
    "        doc_ids.append(doc_id)\n",
    "        training_data.append(text)\n",
    "    return doc_ids, np.array(training_data)\n",
    "\n",
    "def create_and_train_tokenizer(train_file_ids):\n",
    "    text = \"\"\n",
    "    for file_id in train_file_ids:\n",
    "        text += gutenberg.raw(file_id)\n",
    "\n",
    "    trainer = PunktTrainer()\n",
    "    trainer.INCLUDE_ALL_COLLOCS = True\n",
    "    trainer.train(text)\n",
    "    \n",
    "    tokenizer = PunktSentenceTokenizer(trainer.get_params())\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "creating training data/labels: 100%|██████████| 308/308 [00:00<00:00, 164002.24it/s]\n"
     ]
    }
   ],
   "source": [
    "doc_ids, training_data = create_training_data_and_labels(es, 'duc-2001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=10000, stop_words='english')\n",
    "training_vectors = vectorizer.fit_transform(training_data).todense()"
   ]
  },
  {
   "source": [
    "### LDA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=20, max_iter=5, learning_method='online', learning_offset=50, random_state=0)\n",
    "doc_topics_distribution = lda.fit_transform(training_vectors)"
   ]
  },
  {
   "source": [
    "### NMF"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=20, random_state=1, alpha=.1, l1_ratio=.5)\n",
    "doc_topics_distribution = nmf.fit_transform(training_vectors)"
   ]
  },
  {
   "source": [
    "### Update Topics and Top Words"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_top_k_words(k, model, vectorizer, doc_ids):\n",
    "    \"\"\" Yield in JSON format consisting of top k words per topic. \"\"\"\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "    topic_bodies = []\n",
    "    for topic_id, words_prob in enumerate(model.components_):\n",
    "        top_words_indices = words_prob.argsort()[::-1][:k]\n",
    "        top_word_probs = [words_prob[index] for index in top_words_indices]\n",
    "        top_words = [feature_names[index] for index in top_words_indices]\n",
    "\n",
    "        word_bodies = []\n",
    "        for index in range(len(top_words_indices)):\n",
    "            word_bodies.append({\n",
    "                \"word\": top_words[index],\n",
    "                \"probability\": top_word_probs[index]\n",
    "            })\n",
    "\n",
    "        topic_body = {\n",
    "            \"topic_id\": topic_id,\n",
    "            \"top_words\": word_bodies,\n",
    "        }\n",
    "        topic_bodies.append(topic_body)\n",
    "\n",
    "    for doc_id in tqdm(doc_ids, position=0, desc='bulk update top 10 words per topic'):\n",
    "        yield {\n",
    "            '_index': 'duc-2001',\n",
    "            '_op_type': 'update',\n",
    "            '_id': doc_id,\n",
    "            'doc': {\n",
    "                'topic_index': 'duc-2001-topics',\n",
    "                'topics': topic_bodies\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "def update_top_k_topics_per_doc(k, distribution, doc_ids):\n",
    "    \"\"\" Yield in JSON format consisting of top k topics per document. \"\"\"\n",
    "    doc_index = 0\n",
    "    for distribution in tqdm(doc_topics_distribution, position=0, desc='bulk update top topics per document'):\n",
    "        top_topics_indices = distribution.argsort()[::-1][:k]\n",
    "        top_topics_probabilities = [distribution[index] for index in top_topics_indices]\n",
    "\n",
    "        topic_bodies = []\n",
    "        for index in range(len(top_topics_indices)):\n",
    "            topic_bodies.append({\n",
    "                \"topic\": str(top_topics_indices[index]),\n",
    "                \"probability\": top_topics_probabilities[index]\n",
    "            })\n",
    "\n",
    "        doc_id = doc_ids[doc_index]\n",
    "        doc_index += 1\n",
    "\n",
    "        yield {\n",
    "            '_index': 'duc-2001',\n",
    "            '_op_type': 'update',\n",
    "            '_id': doc_id,\n",
    "            'doc': {\n",
    "                'doc_topics': topic_bodies\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "bulk update top 10 words per topic: 100%|██████████| 308/308 [00:00<00:00, 2457.30it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(308, [])"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "bulk(es, update_top_k_words(k=10, model=lda, vectorizer=vectorizer, doc_ids=doc_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "bulk update top topics per document: 100%|██████████| 308/308 [00:00<00:00, 13597.09it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(308, [])"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "bulk(es, update_top_k_topics_per_doc(k=5, distribution=doc_topics_distribution, doc_ids=doc_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}