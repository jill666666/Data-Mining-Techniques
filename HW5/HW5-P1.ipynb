{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python381jvsc74a57bd0aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49",
   "display_name": "Python 3.8.1 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk"
   ]
  },
  {
   "source": [
    "# Problem 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Create Index"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(index_name, stopwords):\n",
    "    es.indices.create(index=index_name, ignore=400, body={\n",
    "        \"settings\" : {\n",
    "            \"number_of_shards\": 1,\n",
    "            \"number_of_replicas\": 1,\n",
    "            \"max_result_window\" : 20000,\n",
    "            \"analysis\": {\n",
    "                \"filter\": {\n",
    "                    \"english_stop\": {\n",
    "                        \"type\": \"stop\",\n",
    "                        \"stopwords\": stopwords\n",
    "                    },\n",
    "                    \"stemmer\": {\n",
    "                        \"type\": \"snowball\",\n",
    "                        \"name\": \"english\"\n",
    "                    }\n",
    "                },\n",
    "                \"analyzer\": {\n",
    "                    \"stopped\": {\n",
    "                        \"type\": \"custom\",\n",
    "                        \"tokenizer\": \"standard\",\n",
    "                        \"filter\": [\n",
    "                            \"lowercase\",\n",
    "                            \"english_stop\",\n",
    "                            \"stemmer\"\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "        }\n",
    "        },\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"text\": {\n",
    "                    \"type\": \"text\",\n",
    "                    \"fielddata\": True,\n",
    "                    \"analyzer\": \"stopped\",\n",
    "                    \"index_options\": \"positions\",\n",
    "                    \"term_vector\": \"yes\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "source": [
    "### Index Documents - DUC 2001 Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_duc(docs, summary_dict):\n",
    "    summary = ''\n",
    "    text_start, text_end = docs.find('<TEXT>') + len('<TEXT>'), docs.find('</TEXT>')\n",
    "    docno_start, docno_end = docs.find('<DOCNO>') + len('<DOCNO>'), docs.find('</DOCNO>')\n",
    "    text = docs[text_start:text_end].strip()\n",
    "    docno = docs[docno_start:docno_end].strip()\n",
    "    if docno in summary_dict.keys():\n",
    "        summary = summary_dict[docno]\n",
    "\n",
    "    yield {\n",
    "        '_index': 'duc-2001',\n",
    "        '_id': docno,\n",
    "        'text': text,\n",
    "        'gold_summary': summary\n",
    "    }\n",
    "\n",
    "def index_duc(filepath):\n",
    "    print('indexing DUC documents')\n",
    "    \n",
    "    summary_dict = {}\n",
    "    summary_files = os.listdir(f'{filepath}/Summaries')\n",
    "    for filename in summary_files:\n",
    "        summary_docno, _ = tuple(filename.split('.'))\n",
    "        with open(f'{filepath}/Summaries/{filename}') as f:\n",
    "            summary_dict[summary_docno.upper()] = f.read()\n",
    "\n",
    "    files = os.listdir(filepath)\n",
    "    for file in tqdm(files, position=0, desc='index duc dataset'):\n",
    "        if '-' in file:\n",
    "            with open(f'{filepath}/{file}', 'r') as f:\n",
    "                docs = f.read()\n",
    "                bulk(es, parse_duc(docs, summary_dict))"
   ]
  },
  {
   "source": [
    "### Index Documents - 20NG Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_20ng(filename, category, text):\n",
    "    yield {\n",
    "        '_index': '20-ng',\n",
    "        '_id': filename,\n",
    "        'category': category,\n",
    "        'text': text\n",
    "        }\n",
    "\n",
    "def index_20ng(filepath):    \n",
    "    folders = os.listdir(f'{filepath}')\n",
    "    valid_folders = [folder for folder in folders if '.' not in folder]\n",
    "    for folder in valid_folders:\n",
    "        categories = os.listdir(f'{filepath}/{folder}')\n",
    "        for category in tqdm(categories, position=0, desc=f\"inspecting all categories from folder '{folder}'\"):\n",
    "            files = os.listdir(f'{filepath}/{folder}/{category}')\n",
    "            for filename in files:\n",
    "                with open(f'{filepath}/{folder}/{category}/{filename}', \"r\", encoding=\"ISO-8859-1\") as f:\n",
    "                    text = f.read()\n",
    "                    bulk(es, parse_20ng(filename, category, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(['https://sunho:Dunkel6eit!!@i-o-optimized-deployment-84c1c6.es.us-east-1.aws.found.io:9243'])\n",
    "stopwords = [line.strip() for line in open('./stoplist.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "indexing DUC documents\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=313.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c89306002cb146b9bafd9d644f350ac7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "create_index('duc-2001', stopwords)\n",
    "index_duc('DUC2001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "inspecting all categories from folder '20news-bydate-test': 100%|██████████| 20/20 [04:32<00:00, 13.64s/it]\n",
      "inspecting all categories from folder '20news-bydate-train': 100%|██████████| 20/20 [06:59<00:00, 20.95s/it]\n"
     ]
    }
   ],
   "source": [
    "create_index('20-ng', stopwords)\n",
    "index_20ng('20NG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'one two'"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "' '.join(['one', 'two'])"
   ]
  }
 ]
}