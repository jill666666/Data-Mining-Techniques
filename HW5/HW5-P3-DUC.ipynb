{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python381jvsc74a57bd0aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49",
   "display_name": "Python 3.8.1 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(['https://sunho:Dunkel6eit!!@i-o-optimized-deployment-84c1c6.es.us-east-1.aws.found.io:9243'], timeout=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data_and_labels(es, index_name):\n",
    "    \"\"\" Create training data/labels given the Elasticsearch client. \"\"\"\n",
    "    doc_ids, training_data, training_labels = [], [], []\n",
    "    res = es.search(index=index_name, size=20000)\n",
    "    hits = res['hits']['hits']\n",
    "    for hit in tqdm(hits, position=0, desc='creating training data/labels'):\n",
    "        doc_id = hit['_id']\n",
    "        text = hit['_source']['text']\n",
    "        doc_ids.append(doc_id)\n",
    "        training_data.append(text)\n",
    "    return doc_ids, np.array(training_data)\n",
    "\n",
    "def create_and_train_tokenizer(train_file_ids):\n",
    "    text = \"\"\n",
    "    for file_id in train_file_ids:\n",
    "        text += gutenberg.raw(file_id)\n",
    "\n",
    "    trainer = PunktTrainer()\n",
    "    trainer.INCLUDE_ALL_COLLOCS = True\n",
    "    trainer.train(text)\n",
    "    \n",
    "    tokenizer = PunktSentenceTokenizer(trainer.get_params())\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "creating training data/labels: 100%|██████████| 308/308 [00:00<00:00, 225295.72it/s]\n"
     ]
    }
   ],
   "source": [
    "doc_ids, training_data = create_training_data_and_labels(es, 'duc-2001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = create_and_train_tokenizer(train_file_ids=gutenberg.fileids())\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_score(p, q, lambda_param=0.1):\n",
    "    \"\"\" Compute KL similarity score. \"\"\"\n",
    "    return sum([p[i] * math.log(p[i] + lambda_param / (q[i] + lambda_param * len(p) ) ) for i in range(len(p))])\n",
    "\n",
    "def KL_similarization(training_data, vectorizer, tokenizer):\n",
    "    \"\"\" Extract KL summaries given the training dataset. \"\"\"\n",
    "    summaries = {}\n",
    "\n",
    "    for doc_index, document in enumerate(tqdm(training_data, position=0, desc='KL summarization')):\n",
    "        best_sentences = []\n",
    "        vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        doc_vectors = vectorizer.fit_transform([document]).toarray()[0]\n",
    "        for _ in range(3):\n",
    "            sentences = tokenizer.tokenize(document)\n",
    "            sentence_score_map = {}\n",
    "            for sentence_order, sentence in enumerate(sentences):\n",
    "                if sentence in best_sentences:\n",
    "                    continue\n",
    "                candidates = [summary for summary in best_sentences]\n",
    "                candidates.append(sentence)\n",
    "                candidate_as_sentences = ' '.join(candidate for candidate in candidates)\n",
    "                candidate_vector = vectorizer.transform([candidate_as_sentences]).toarray()[0]\n",
    "                sentence_score_map[sentence_order] = kl_score(doc_vectors, candidate_vector)\n",
    "            if len(sentence_score_map) != 0:\n",
    "                top_candidate_order, top_candidate_score = sorted(sentence_score_map.items(), key=lambda x: x[1])[0]\n",
    "                top_candidate_sentence = sentences[top_candidate_order]\n",
    "                best_sentences.append(top_candidate_sentence)\n",
    "        \n",
    "        doc_id = doc_ids[doc_index]\n",
    "        summary = ' '.join(best_sentences)\n",
    "        summaries[doc_id] = summary\n",
    "    return summaries\n",
    "\n",
    "def LDA_summarization(training_data, vectorizer, tokenizer):\n",
    "    \"\"\" Extract LDA summaries given the training dataset. \"\"\"\n",
    "    summaries = {}\n",
    "    lda = LatentDirichletAllocation(n_components=20, max_iter=5, learning_method='online', learning_offset=50, random_state=0)\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        for doc_index, document in enumerate(tqdm(training_data, position=0, desc='LDA summarization')):\n",
    "            best_sentences = []\n",
    "            vectorizer = TfidfVectorizer(stop_words='english')\n",
    "            doc_vectors = vectorizer.fit_transform([document]).todense()\n",
    "            doc_topics_distribution = lda.fit_transform(doc_vectors).flatten()\n",
    "            for _ in range(3):\n",
    "                sentences = tokenizer.tokenize(document)\n",
    "                sentence_score_map = {}\n",
    "                for sentence_order, sentence in enumerate(sentences):\n",
    "                    if sentence in best_sentences:\n",
    "                        continue\n",
    "                    candidates = [summary for summary in best_sentences]\n",
    "                    candidates.append(sentence)\n",
    "                    candidate_as_sentences = ' '.join(candidate for candidate in candidates)\n",
    "                    candidate_vector = vectorizer.transform([candidate_as_sentences]).todense()\n",
    "                    sentence_topics_distribution = lda.transform(candidate_vector).flatten()\n",
    "                    sentence_score_map[sentence_order] = kl_score(doc_topics_distribution, sentence_topics_distribution)\n",
    "                if len(sentence_score_map) == 0:\n",
    "                    continue\n",
    "                top_candidate_order, top_candidate_score = sorted(sentence_score_map.items(), key=lambda x: x[1])[0]\n",
    "                top_candidate_sentence = sentences[top_candidate_order]\n",
    "                best_sentences.append(top_candidate_sentence)\n",
    "            \n",
    "            doc_id = doc_ids[doc_index]\n",
    "            summary = ' '.join(best_sentences)\n",
    "            summaries[doc_id] = summary\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "KL summarization: 100%|██████████| 308/308 [00:49<00:00,  6.22it/s]\n"
     ]
    }
   ],
   "source": [
    "kl_summaries = KL_similarization(training_data, vectorizer, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "LDA summarization: 100%|██████████| 308/308 [00:50<00:00,  6.06it/s]\n"
     ]
    }
   ],
   "source": [
    "lda_summaries = LDA_summarization(training_data, vectorizer, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_summaries(index_name, kl_summaries, lda_summaries):\n",
    "    summaries_zipped = zip(kl_summaries.items(), lda_summaries.items())\n",
    "    for (doc_id, kl_summary), (_, lda_summary) in tqdm(summaries_zipped, position=0, desc=f'updating {index_name} summaries'):\n",
    "        yield {\n",
    "            '_index': index_name,\n",
    "            '_op_type': 'update',\n",
    "            '_id': doc_id,\n",
    "            'doc': {\n",
    "                'kl_summary': kl_summary,\n",
    "                'lda_summary': lda_summary\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "updating duc-2001 summaries: 308it [00:00, 22794.73it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(308, [])"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "bulk(es, update_summaries('duc-2001', kl_summaries, lda_summaries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrouge.rouge import Rouge155\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluation(summaries):\n",
    "    res = es.search(index='duc-2001')\n",
    "    hits = res['hits']['hits']\n",
    "    count, precision, recall, f_score = 0, 0, 0, 0\n",
    "    for hit in hits:\n",
    "        summary_id = hit['_id']\n",
    "        summary_text = hit['_source']['gold_summary']\n",
    "\n",
    "        ref_texts = {summary_id: summaries[summary_id]}\n",
    "\n",
    "        rouge = Rouge155(n_words=100)\n",
    "        score_dict = rouge.score_summary(summary_text, ref_texts)\n",
    "        precision += score_dict['rouge_1_precision']\n",
    "        recall += score_dict['rouge_1_recall']\n",
    "        f_score += score_dict['rouge_1_f_score']\n",
    "        count += 1\n",
    "    precision, recall, f_score = precision/count, recall/count, f_score/count\n",
    "    print(f'rouge_1_precision: {precision}, rouge_1_recall: {recall}, rouge_1_f_score: {f_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "KL summaries evaluation\n",
      "rouge_1_precision: 0.791975, rouge_1_recall: 0.795583, rouge_1_f_score: 0.7936829999999999\n"
     ]
    }
   ],
   "source": [
    "print('KL summaries evaluation')\n",
    "evaluation(kl_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LDA summaries evaluation\n",
      "rouge_1_precision: 0.773509, rouge_1_recall: 0.7840779999999999, rouge_1_f_score: 0.7786629999999999\n"
     ]
    }
   ],
   "source": [
    "print('LDA summaries evaluation')\n",
    "evaluation(lda_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'rouge_1_f_score': 0.77586,\n 'rouge_1_f_score_cb': 0.77586,\n 'rouge_1_f_score_ce': 0.77586,\n 'rouge_1_precision': 0.88235,\n 'rouge_1_precision_cb': 0.88235,\n 'rouge_1_precision_ce': 0.88235,\n 'rouge_1_recall': 0.69231,\n 'rouge_1_recall_cb': 0.69231,\n 'rouge_1_recall_ce': 0.69231,\n 'rouge_2_f_score': 0.57894,\n 'rouge_2_f_score_cb': 0.57894,\n 'rouge_2_f_score_ce': 0.57894,\n 'rouge_2_precision': 0.66,\n 'rouge_2_precision_cb': 0.66,\n 'rouge_2_precision_ce': 0.66,\n 'rouge_2_recall': 0.51562,\n 'rouge_2_recall_cb': 0.51562,\n 'rouge_2_recall_ce': 0.51562,\n 'rouge_3_f_score': 0.5,\n 'rouge_3_f_score_cb': 0.5,\n 'rouge_3_f_score_ce': 0.5,\n 'rouge_3_precision': 0.57143,\n 'rouge_3_precision_cb': 0.57143,\n 'rouge_3_precision_ce': 0.57143,\n 'rouge_3_recall': 0.44444,\n 'rouge_3_recall_cb': 0.44444,\n 'rouge_3_recall_ce': 0.44444,\n 'rouge_4_f_score': 0.45455,\n 'rouge_4_f_score_cb': 0.45455,\n 'rouge_4_f_score_ce': 0.45455,\n 'rouge_4_precision': 0.52083,\n 'rouge_4_precision_cb': 0.52083,\n 'rouge_4_precision_ce': 0.52083,\n 'rouge_4_recall': 0.40323,\n 'rouge_4_recall_cb': 0.40323,\n 'rouge_4_recall_ce': 0.40323,\n 'rouge_su4_f_score': 0.64759,\n 'rouge_su4_f_score_cb': 0.64759,\n 'rouge_su4_f_score_ce': 0.64759,\n 'rouge_su4_precision': 0.74138,\n 'rouge_su4_precision_cb': 0.74138,\n 'rouge_su4_precision_ce': 0.74138,\n 'rouge_su4_recall': 0.57487,\n 'rouge_su4_recall_cb': 0.57487,\n 'rouge_su4_recall_ce': 0.57487}\n"
     ]
    }
   ],
   "source": [
    "from pyrouge.rouge import Rouge155\n",
    "from pprint import pprint\n",
    "\n",
    "ref_texts = {'A': \"Poor nations pressurise developed countries into granting trade subsidies.\"}\n",
    "summary_text = \"Poor nations demand trade subsidies from developed nations.\"\n",
    "\n",
    "\n",
    "rouge = Rouge155(n_words=100)\n",
    "score = rouge.score_summary(summary_text, ref_texts)\n",
    "pprint(score)"
   ]
  }
 ]
}